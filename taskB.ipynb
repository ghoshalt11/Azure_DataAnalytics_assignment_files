{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b33cc840-236e-4169-a877-4da1b394794d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# setting up config using storage account level SAS token access\n",
    "\n",
    "account_name = \"sttg1smartgear\"     # My storage account on Azure\n",
    "sas_token = \"sv=2024-11-04&ss=b&srt=sco&sp=rwlcyx&se=2025-09-08T00:30:08Z&st=2025-09-07T16:15:08Z&spr=https&sig=zo1CiQrHS%2BQT0fUjhK4CSAmaddGMQvbcUUbow%2Bjvu80%3D\"\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{account_name}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{account_name}.dfs.core.windows.net\",\n",
    "               \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{account_name}.dfs.core.windows.net\", sas_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed66d042-79cb-476c-a49a-507458bff0fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Mounts storage\n",
    "input_container = \"rawdata\"\n",
    "curated_container=\"curated\"\n",
    "input_file_path = f\"abfss://{input_container}@{account_name}.dfs.core.windows.net/raw/smartgear_sales.csv\"\n",
    "output_file_path = f\"abfss://{curated_container}@{account_name}.dfs.core.windows.net/region_qty.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec3e0c99-2c0b-412a-bdc4-756f75d5b439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-------+----------+--------+---------+\n|OrderID| OrderDate|Region|StoreID|   Product|Quantity|UnitPrice|\n+-------+----------+------+-------+----------+--------+---------+\n|   1001|2025-03-14|  East|    115|Headphones|       3|    81.65|\n|   1002|2025-02-19|  West|    110|Smartwatch|       3|    242.5|\n|   1003|2025-03-17|  West|    113|   Printer|       2|   152.59|\n|   1004|2025-01-05|  West|    118|    Camera|       4|    463.4|\n|   1005|2025-01-07|  West|    110|    Tablet|       3|   370.01|\n|   1006|2025-03-22|  East|    113|Smartphone|       4|   639.47|\n|   1007|2025-02-19| North|    118|     Drone|       4|   747.21|\n|   1008|2025-02-21|  West|    112|   Printer|       2|   151.06|\n|   1009|2025-03-30| North|    117|Smartphone|       1|   614.89|\n|   1010|2025-01-30|  West|    108|    Tablet|       3|   415.43|\n+-------+----------+------+-------+----------+--------+---------+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# 2. Reads the CSV with spark.read.csv\n",
    "df = (spark.read\n",
    "      .option(\"header\", \"true\")     \n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .csv(input_file_path))\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9eed29b-7348-4c65-bb70-5793824626f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n|Region|total_quantity|\n+------+--------------+\n| North|           848|\n| South|           740|\n|  East|           719|\n|  West|           694|\n+------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 3.  Aggregating total Quantity by Region\n",
    "import pyspark.sql.functions as func\n",
    "region_qty = (df.groupBy(\"Region\")\n",
    "                .agg(func.sum(\"Quantity\").alias(\"total_quantity\"))\n",
    "                .orderBy(func.desc(\"total_quantity\")))\n",
    "\n",
    "region_qty.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e2fc6d2-afa7-47c3-bd5f-6c5d45849a69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output result saved as region_qty.parquet in a container named - curated\n"
     ]
    }
   ],
   "source": [
    "# writing result output from dataframe to parquet\n",
    "region_qty.coalesce(1).write.mode(\"overwrite\").parquet(output_file_path)\n",
    "print(\"output result saved as region_qty.parquet in a container named - curated\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "taskB",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}